#!/usr/bin/env python3
"""
ElasticSearch ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ (SRS v1.1)

ê¸°ì¡´ ë°ì´í„°ì˜ í•„ë“œëª…ì„ ì—…ë°ì´íŠ¸:
- url â†’ source_url
- source â†’ source_name (ì´ë¯¸ sourceê°€ ìˆëŠ” ê²½ìš°)
- sentiment.classification â†’ lowercase ë³€í™˜
- analyzed_date, metadata í•„ë“œ ì¶”ê°€
"""

import sys
import os
from datetime import datetime, timezone

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from elasticsearch import Elasticsearch


def migrate_documents():
    """ê¸°ì¡´ ë¬¸ì„œë¥¼ SRS v1.1 ìŠ¤í‚¤ë§ˆë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
    
    es = Elasticsearch(['http://localhost:9200'])
    index_name = 'news_analysis'
    
    # ì—°ê²° í™•ì¸
    if not es.ping():
        print("âŒ ElasticSearch ì—°ê²° ì‹¤íŒ¨")
        return False
    
    print("âœ“ ElasticSearch ì—°ê²° ì„±ê³µ")
    
    # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ
    try:
        result = es.search(
            index=index_name,
            query={'match_all': {}},
            size=1000
        )
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
        return False
    
    total = result['hits']['total']['value']
    print(f"\nğŸ“Š ì´ {total}ê°œ ë¬¸ì„œ ë°œê²¬")
    
    if total == 0:
        print("ë§ˆì´ê·¸ë ˆì´ì…˜í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return True
    
    updated_count = 0
    error_count = 0
    
    for hit in result['hits']['hits']:
        doc_id = hit['_id']
        doc = hit['_source']
        
        try:
            updates = {}
            
            # 1. url â†’ source_url ë³€í™˜
            if 'url' in doc and 'source_url' not in doc:
                updates['source_url'] = doc['url']
            
            # 2. source â†’ source_name ë³€í™˜
            if 'source' in doc and 'source_name' not in doc:
                # sourceê°€ URLì¸ì§€ ì´ë¦„ì¸ì§€ í™•ì¸
                source_val = doc['source']
                if source_val.startswith('http'):
                    # URLì¸ ê²½ìš° source_urlë¡œ ì´ë™
                    if 'source_url' not in updates:
                        updates['source_url'] = source_val
                    updates['source_name'] = 'Investing.com'
                else:
                    updates['source_name'] = source_val.title()  # 'investing.com' â†’ 'Investing.Com'
            
            # source_name ê¸°ë³¸ê°’ ì„¤ì •
            if 'source_name' not in doc and 'source_name' not in updates:
                updates['source_name'] = 'Investing.com'
            
            # 3. sentiment.classification lowercase ë³€í™˜
            sentiment = doc.get('sentiment', {})
            if sentiment:
                classification = sentiment.get('classification', '')
                if classification and classification != classification.lower():
                    updates['sentiment'] = {
                        'classification': classification.lower(),
                        'score': sentiment.get('score', 0)
                    }
            
            # 4. analyzed_date ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
            if 'analyzed_date' not in doc:
                # crawled_date ê¸°ë°˜ìœ¼ë¡œ ì¶”ì •
                crawled = doc.get('crawled_date')
                if crawled:
                    updates['analyzed_date'] = crawled
                else:
                    updates['analyzed_date'] = datetime.now(timezone.utc).isoformat()
            
            # 5. metadata ì¶”ê°€ (ì—†ëŠ” ê²½ìš°)
            if 'metadata' not in doc:
                content = doc.get('content', '')
                updates['metadata'] = {
                    'word_count': len(content.split()) if content else 0,
                    'language': 'en',
                    'gpt_model': 'gpt-4'
                }
            
            # ì—…ë°ì´íŠ¸ ì‹¤í–‰
            if updates:
                es.update(
                    index=index_name,
                    id=doc_id,
                    doc=updates
                )
                updated_count += 1
                print(f"  âœ“ {doc_id[:8]}... ì—…ë°ì´íŠ¸ ì™„ë£Œ (fields: {list(updates.keys())})")
            else:
                print(f"  - {doc_id[:8]}... ë³€ê²½ ì—†ìŒ")
                
        except Exception as e:
            error_count += 1
            print(f"  âŒ {doc_id[:8]}... ì˜¤ë¥˜: {e}")
    
    # ì˜¤ë˜ëœ í•„ë“œ ì‚­ì œ (painless script ì‚¬ìš©)
    print("\nğŸ§¹ ì˜¤ë˜ëœ í•„ë“œ ì •ë¦¬ ì¤‘...")
    
    try:
        # url, source í•„ë“œ ì‚­ì œ
        es.update_by_query(
            index=index_name,
            body={
                "script": {
                    "source": """
                        if (ctx._source.containsKey('url')) {
                            ctx._source.remove('url');
                        }
                        if (ctx._source.containsKey('source')) {
                            ctx._source.remove('source');
                        }
                    """,
                    "lang": "painless"
                },
                "query": {
                    "bool": {
                        "should": [
                            {"exists": {"field": "url"}},
                            {"exists": {"field": "source"}}
                        ]
                    }
                }
            }
        )
        print("  âœ“ ì˜¤ë˜ëœ í•„ë“œ ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"  âš  í•„ë“œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œ ê°€ëŠ¥): {e}")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print(f"ğŸ“‹ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
    print(f"   - ì´ ë¬¸ì„œ: {total}")
    print(f"   - ì—…ë°ì´íŠ¸: {updated_count}")
    print(f"   - ì˜¤ë¥˜: {error_count}")
    print(f"{'='*50}")
    
    return error_count == 0


def verify_migration():
    """ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦"""
    
    es = Elasticsearch(['http://localhost:9200'])
    index_name = 'news_analysis'
    
    print("\nğŸ” ë§ˆì´ê·¸ë ˆì´ì…˜ ê²°ê³¼ ê²€ì¦...")
    
    # ìƒ˜í”Œ ë¬¸ì„œ ì¡°íšŒ
    result = es.search(
        index=index_name,
        query={'match_all': {}},
        size=3
    )
    
    print("\nìƒ˜í”Œ ë¬¸ì„œ êµ¬ì¡°:")
    for i, hit in enumerate(result['hits']['hits'][:3]):
        doc = hit['_source']
        print(f"\n--- Document {i+1} ---")
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_v11 = ['source_url', 'source_name', 'analyzed_date', 'metadata']
        removed = ['url', 'source']
        
        for field in required_v11:
            if field in doc:
                val = doc[field]
                if isinstance(val, dict):
                    print(f"  âœ“ {field}: {list(val.keys())}")
                elif isinstance(val, str) and len(val) > 50:
                    print(f"  âœ“ {field}: \"{val[:50]}...\"")
                else:
                    print(f"  âœ“ {field}: {val}")
            else:
                print(f"  âŒ {field}: (ëˆ„ë½)")
        
        for field in removed:
            if field in doc:
                print(f"  âš  {field}: (ì‚­ì œë˜ì§€ ì•ŠìŒ)")
        
        # sentiment classification í™•ì¸
        sentiment = doc.get('sentiment', {})
        classification = sentiment.get('classification', '')
        if classification:
            if classification == classification.lower():
                print(f"  âœ“ sentiment.classification: {classification} (lowercase)")
            else:
                print(f"  âš  sentiment.classification: {classification} (ëŒ€ë¬¸ì í¬í•¨)")


if __name__ == '__main__':
    print("=" * 50)
    print("ElasticSearch ìŠ¤í‚¤ë§ˆ ë§ˆì´ê·¸ë ˆì´ì…˜ (SRS v1.1)")
    print("=" * 50)
    
    success = migrate_documents()
    
    if success:
        verify_migration()
    
    print("\nì™„ë£Œ!")
